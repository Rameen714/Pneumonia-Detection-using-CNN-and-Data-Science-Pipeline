{"cells":[{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","kaggle_directory1 = '/kaggle/input/datascienceproject4/chest_xray/train'\n","kaggle_directory2 = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\n","local = './chest_xray/train'\n","local_valid = './chest_xray/val'\n","local_test = './chest_xray/test'\n","normal_directory = os.path.join(local,'NORMAL' )\n","pneumonia_directory = os.path.join(local,'PNEUMONIA' )\n","cleaned_data_directory = './cleaned_data'\n","edgedImages_data_directory = './edge_images'\n","normalizedImages_data_directory = './normalized_images_equal_ratio'\n","normalizedImages_data_directory_valid = './normalized_images_equal_ratio_valid'\n","normalizedImages_data_directory_test = './normalized_images_equal_ratio_test'"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T22:40:43.572046Z","iopub.status.busy":"2024-05-11T22:40:43.570953Z","iopub.status.idle":"2024-05-11T22:42:15.577200Z","shell.execute_reply":"2024-05-11T22:42:15.576237Z","shell.execute_reply.started":"2024-05-11T22:40:43.572010Z"},"trusted":true},"outputs":[],"source":["# Function to check for missing values in an image\n","def check_missing_values(image_path):\n","    # Load image\n","    image = cv2.imread(image_path)\n","\n","    # Check for missing values (e.g., NaN or None)\n","    missing_values = np.isnan(image).sum()  # Example for NumPy array\n","    if missing_values > 0:\n","        print(f\"Missing values found in image: {image_path}\")\n","    else:\n","        print(f\"No missing values found in image: {image_path}\")\n","\n","# List all files under the input directory\n","for dirname, _, filenames in os.walk(local):\n","    for filename in filenames:\n","        image_path = os.path.join(dirname, filename)\n","        check_missing_values(image_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:24:55.590330Z","iopub.status.busy":"2024-05-11T22:24:55.589530Z","iopub.status.idle":"2024-05-11T22:24:55.597137Z","shell.execute_reply":"2024-05-11T22:24:55.596071Z","shell.execute_reply.started":"2024-05-11T22:24:55.590300Z"},"trusted":true},"outputs":[],"source":["# Function to check for missing values in an image and plot histogram\n","def check_missing_values_and_plot_histogram(image_path):\n","    # Load image\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Check for missing values (e.g., NaN or None)\n","    missing_values = np.isnan(image).sum()  # Example for NumPy array\n","    if missing_values > 0:\n","        print(f\"Missing values found in image: {image_path}\")\n","    else:\n","        print(f\"No missing values found in image: {image_path}\")\n","\n","    # Plot histogram of pixel intensities\n","    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n","    plt.plot(hist, color='black')\n","    plt.xlabel('Pixel Intensity')\n","    plt.ylabel('Frequency')\n","    plt.title('Histogram of Pixel Intensities')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:25:08.589474Z","iopub.status.busy":"2024-05-11T22:25:08.589148Z","iopub.status.idle":"2024-05-11T22:25:08.804801Z","shell.execute_reply":"2024-05-11T22:25:08.803966Z","shell.execute_reply.started":"2024-05-11T22:25:08.589445Z"},"trusted":true},"outputs":[],"source":["check_missing_values_and_plot_histogram(local)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#An image histogram is a type of histogram that acts as a \n","#graphical representation of the tonal distribution in a \n","#digital image. It plots the number of pixels for each tonal value. \n","#By looking at the histogram for a specific image a viewer will be able\n","#to judge the entire tonal distribution at a glance.\n","#The horizontal axis of the graph represents \n","#the tonal variations, while the vertical axis represents \n","#the total number of pixels in that particular tone.[1]"]},{"cell_type":"markdown","metadata":{},"source":["**Combined Histogram**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:39:10.147560Z","iopub.status.busy":"2024-05-11T22:39:10.146832Z","iopub.status.idle":"2024-05-11T22:40:22.947507Z","shell.execute_reply":"2024-05-11T22:40:22.946502Z","shell.execute_reply.started":"2024-05-11T22:39:10.147523Z"},"trusted":true},"outputs":[],"source":["# Function to accumulate histograms of all images in a directory\n","def accumulate_histograms(directory):\n","    # Initialize an empty list to store histograms\n","    histograms = []\n","\n","    # List all files under the specified directory\n","    for dirname, _, filenames in os.walk(directory):\n","        for filename in filenames:\n","            # Load image\n","            image_path = os.path.join(dirname, filename)\n","            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","            # Calculate histogram\n","            hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n","\n","            # Append histogram to the list\n","            histograms.append(hist)\n","\n","    # Combine histograms using numpy sum function along axis 0\n","    combined_hist = np.sum(histograms, axis=0)\n","\n","    return combined_hist\n","\n","# Function to plot a histogram\n","def plot_histogram(hist, data_name):\n","    plt.plot(hist, color='black')\n","    plt.xlabel('Pixel Intensity')\n","    plt.ylabel('Frequency')\n","    plt.title('Histogram of Pixel Intensities' + ' of ' + data_name + ' data set')\n","    plt.show()\n","\n","# Directory containing the image data\n","# data_directory = '/kaggle/input/datascienceproject4/chest_xray/train'\n","\n","# Accumulate histograms of all images\n","combined_hist = accumulate_histograms(normal_directory)\n","# Plot the combined histogram\n","plot_histogram(combined_hist, 'NORMAL')\n","\n","# Accumulate histograms of all images\n","combined_hist = accumulate_histograms(pneumonia_directory)\n","# Plot the combined histogram\n","plot_histogram(combined_hist, 'PNEUMONIA')\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:47:59.225251Z","iopub.status.busy":"2024-05-11T22:47:59.224990Z","iopub.status.idle":"2024-05-11T22:48:01.700677Z","shell.execute_reply":"2024-05-11T22:48:01.699796Z","shell.execute_reply.started":"2024-05-11T22:47:59.225230Z"},"trusted":true},"outputs":[],"source":["# Define the subdirectories for normal and pneumonia-infected individuals\n","# normal_directory = os.path.join(data_directory, 'NORMAL')\n","# pneumonia_directory = os.path.join(data_directory, 'PNEUMONIA')\n","\n","# Define the number of sample images to display from each category\n","num_samples = 5\n","\n","# Function to display sample images\n","def display_sample_images(directory, label):\n","    print(f\"Sample images for {label} category:\")\n","    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n","    for i, filename in enumerate(os.listdir(directory)[:num_samples]):\n","        image_path = os.path.join(directory, filename)\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        axes[i].imshow(image)\n","        axes[i].axis('off')\n","    plt.show()\n","\n","# Display sample images for normal and pneumonia categories\n","display_sample_images(normal_directory, 'NORMAL')\n","display_sample_images(pneumonia_directory, 'PNEUMONIA')\n"]},{"cell_type":"markdown","metadata":{},"source":["**Data Cleaning**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from PIL import Image\n","import imagehash\n","import shutil\n","\n","# Define the directory containing the original data\n","# original_data_directory = '/kaggle/input/datascienceproject4/chest_xray/train'\n","\n","# Define the directory for the cleaned data\n","# cleaned_data_directory = '/kaggle/working/cleaned_data'\n","cleaned_data_directory = './cleaned_data'\n","\n","# Copy the original data to the cleaned data directory\n","# shutil.copytree(original_data_directory, cleaned_data_directory)\n","\n","shutil.copytree(local, cleaned_data_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_files_in_directories(directory):\n","    try:\n","        for subdir, _, files in os.walk(directory):\n","            print(f\"Directory: {subdir}, Number of files: {len(files)}\")\n","    except FileNotFoundError:\n","        print(\"Directory not found.\")\n","\n","# Function to remove duplicate images\n","def remove_duplicates(directory):\n","    hash_dict = {}\n","    duplicates = []\n","\n","    for root, _, filenames in os.walk(directory):\n","        for filename in filenames:\n","            image_path = os.path.join(root, filename)\n","            with Image.open(image_path) as img:\n","                hash_value = str(imagehash.average_hash(img))\n","            if hash_value in hash_dict:\n","                duplicates.append(image_path)\n","            else:\n","                hash_dict[hash_value] = image_path\n","\n","    # Remove duplicate images\n","    for duplicate in duplicates:\n","        os.remove(duplicate)\n","\n","# Function to remove samples with missing images\n","def remove_missing_samples(directory):\n","    for root, _, filenames in os.walk(directory):\n","        for filename in filenames:\n","            image_path = os.path.join(root, filename)\n","            if not os.path.exists(image_path):\n","                os.remove(image_path)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:53:23.231694Z","iopub.status.busy":"2024-05-11T22:53:23.230956Z","iopub.status.idle":"2024-05-11T22:54:37.024704Z","shell.execute_reply":"2024-05-11T22:54:37.023866Z","shell.execute_reply.started":"2024-05-11T22:53:23.231663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Count of files BEFORE removing missing sample and duplicates\n","Directory: ./cleaned_data, Number of files: 0\n","Directory: ./cleaned_data\\NORMAL, Number of files: 1260\n","Directory: ./cleaned_data\\PNEUMONIA, Number of files: 3702\n","Count of files after removing duplicates\n","Directory: ./cleaned_data, Number of files: 0\n","Directory: ./cleaned_data\\NORMAL, Number of files: 1260\n","Directory: ./cleaned_data\\PNEUMONIA, Number of files: 3702\n","Count of files after removing missing sample\n","Directory: ./cleaned_data, Number of files: 0\n","Directory: ./cleaned_data\\NORMAL, Number of files: 1260\n","Directory: ./cleaned_data\\PNEUMONIA, Number of files: 3702\n"]}],"source":["print(\"Count of files BEFORE removing missing sample and duplicates\")\n","count_files_in_directories(cleaned_data_directory)\n","\n","# Remove duplicate images from the cleaned data directory\n","remove_duplicates(cleaned_data_directory)\n","print(\"Count of files after removing duplicates\")\n","count_files_in_directories(cleaned_data_directory)\n","\n","# Remove samples with missing images from the cleaned data directory\n","remove_missing_samples(cleaned_data_directory)\n","print(\"Count of files after removing missing sample\")\n","count_files_in_directories(cleaned_data_directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T17:57:40.217888Z","iopub.status.busy":"2024-05-11T17:57:40.217456Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","# Function to apply Adaptive Histogram Equalization (AHE) to an image\n","def apply_ahe(image):\n","    # Convert image to grayscale if not already\n","    if len(image.shape) > 2:\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    \n","    # Apply Adaptive Histogram Equalization (AHE)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    enhanced_image = clahe.apply(image)\n","    \n","    return enhanced_image\n","\n","# Function to apply Contrast-Limited Adaptive Histogram Equalization (CLAHE) to an image\n","def apply_clahe(image):\n","    # Convert image to grayscale if not already\n","    if len(image.shape) > 2:\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    \n","    # Apply Contrast-Limited Adaptive Histogram Equalization (CLAHE)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    enhanced_image = clahe.apply(image)\n","    \n","    return enhanced_image\n","\n","# # List all files under the input directory\n","# for dirname, _, filenames in os.walk('/kaggle/working/cleaned_data'):\n","#     for filename in filenames:\n","#         image_path = os.path.join(dirname, filename)\n","        \n","#         # Load image\n","#         image = cv2.imread(image_path)\n","        \n","#         # Apply AHE and CLAHE\n","#         ahe_image = apply_ahe(image)\n","#         clahe_image = apply_clahe(image)\n","        \n","#         # Plot original and enhanced images\n","#         plt.figure(figsize=(10, 4))\n","#         plt.subplot(1, 3, 1)\n","#         plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","#         plt.title('Original Image')\n","#         plt.axis('off')\n","        \n","#         plt.subplot(1, 3, 2)\n","#         plt.imshow(ahe_image, cmap='gray')\n","#         plt.title('AHE')\n","#         plt.axis('off')\n","        \n","#         plt.subplot(1, 3, 3)\n","#         plt.imshow(clahe_image, cmap='gray')\n","#         plt.title('CLAHE')\n","#         plt.axis('off')\n","        \n","#         plt.show()\n","\n","\n","for subdir in ['normal', 'pneumonia']:\n","    print(f\"Random files from {subdir}:\")\n","    \n","    # Get all file names from the current subdirectory\n","    files = os.listdir(os.path.join(cleaned_data_directory, subdir))\n","    \n","    # Randomly select 10 files\n","    random_files = random.sample(files, min(5, len(files)))\n","    \n","    for filename in random_files:\n","        # Construct the full path to the image\n","        image_path = os.path.join(cleaned_data_directory, subdir, filename)\n","        \n","        # Load image\n","        image = cv2.imread(image_path)\n","        \n","        # Apply AHE and CLAHE\n","        ahe_image = apply_ahe(image)\n","        clahe_image = apply_clahe(image)\n","        \n","        # Plot original and enhanced images\n","        plt.figure(figsize=(10, 4))\n","        \n","        plt.subplot(1, 3, 1)\n","        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        plt.title('Original Image')\n","        plt.axis('off')\n","        \n","        plt.subplot(1, 3, 2)\n","        plt.imshow(ahe_image, cmap='gray')\n","        plt.title('AHE')\n","        plt.axis('off')\n","        \n","        plt.subplot(1, 3, 3)\n","        plt.imshow(clahe_image, cmap='gray')\n","        plt.title('CLAHE')\n","        plt.axis('off')\n","        \n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Edge Detection Techniques**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T23:00:54.407702Z","iopub.status.busy":"2024-05-11T23:00:54.407351Z","iopub.status.idle":"2024-05-11T23:03:22.464761Z","shell.execute_reply":"2024-05-11T23:03:22.463961Z","shell.execute_reply.started":"2024-05-11T23:00:54.407674Z"},"trusted":true},"outputs":[],"source":["# Function to apply Canny edge detection to an image\n","def apply_canny_edge_detection(image):\n","    # Convert image to grayscale if not already\n","    if len(image.shape) > 2:\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","        gray_image = image\n","\n","    # Apply Canny edge detection\n","    edges = cv2.Canny(gray_image, threshold1=30, threshold2=100)  # Adjust thresholds as needed\n","\n","    return edges\n","\n","# Function to list all files under a directory\n","def list_files(directory):\n","    files = []\n","    for dirname, _, filenames in os.walk(directory):\n","        for filename in filenames:\n","            files.append(os.path.join(dirname, filename))\n","    return files\n","\n","# Define the directory containing the data\n","# data_directory = '/kaggle/working/cleaned_data'\n","\n","# List all files under the input directory\n","# data_files = list_files(data_directory)\n","data_files = list_files(cleaned_data_directory)\n","\n","# Define the output directory for edge-detected images\n","# output_dir = '/kaggle/working/edge_images'\n","output_dir = edgedImages_data_directory\n","\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Iterate over each image in the data directory\n","for image_path in data_files:\n","    # Load the image\n","    image = cv2.imread(image_path)\n","    \n","    # Apply Canny edge detection\n","    edges = apply_canny_edge_detection(image)\n","    \n","    # Save the edge-detected image\n","    # filename = os.path.basename(image_path)\n","    # cv2.imwrite(os.path.join(output_dir, filename), edges)\n","\n","    # Determine the parent folder name (normal or pneumonia)\n","    parent_folder = os.path.basename(os.path.dirname(image_path))\n","    \n","    # Create the corresponding output directory if it doesn't exist\n","    output_subdir = os.path.join(output_dir, parent_folder)\n","    os.makedirs(output_subdir, exist_ok=True)\n","    \n","    # Save the edge-detected image\n","    filename = os.path.basename(image_path)\n","    output_path = os.path.join(output_subdir, filename)\n","    cv2.imwrite(output_path, edges)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Image Pre-Processing - Normalization**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T23:13:35.880765Z","iopub.status.busy":"2024-05-11T23:13:35.880419Z","iopub.status.idle":"2024-05-11T23:14:41.777644Z","shell.execute_reply":"2024-05-11T23:14:41.776691Z","shell.execute_reply.started":"2024-05-11T23:13:35.880737Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Image Normalization completed.\n"]}],"source":["# Define directories\n","# input_directory = '/kaggle/input/datascienceproject4/chest_xray/train'\n","# output_directory = '/kaggle/working/preprocessed_images'\n","input_directory = local_test\n","output_directory = normalizedImages_data_directory_test\n","\n","# Create output directory if it doesn't exist\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# Function to resize and normalize images\n","def preprocess_image(image_path, output_directory, target_size=(224, 224)):\n","    # Read image\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    \n","    # Resize image\n","    image = cv2.resize(image, target_size)\n","    \n","    # Normalize pixel values\n","    image = image / 255.0\n","    \n","    # Save preprocessed image\n","    # filename = os.path.basename(image_path)\n","    # output_path = os.path.join(output_directory, filename)\n","    # cv2.imwrite(output_path, image * 255.0)  # Save normalized image\n","\n","    # Determine the parent folder name (normal or pneumonia)\n","    parent_folder = os.path.basename(os.path.dirname(image_path))\n","    \n","    # Create the corresponding output directory if it doesn't exist\n","    output_subdir = os.path.join(output_directory, parent_folder)\n","    os.makedirs(output_subdir, exist_ok=True)\n","    \n","    # Save preprocessed image    \n","    filename = os.path.basename(image_path)\n","    output_path = os.path.join(output_subdir, filename)\n","    cv2.imwrite(output_path, image * 255.0)\n","    \n","# Process images in the input directory\n","for root, _, filenames in os.walk(input_directory):\n","    for filename in filenames:\n","        image_path = os.path.join(root, filename)\n","        preprocess_image(image_path, output_directory)\n","\n","print(\"Image Normalization completed.\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T23:37:36.601480Z","iopub.status.busy":"2024-05-11T23:37:36.601118Z","iopub.status.idle":"2024-05-11T23:37:36.947921Z","shell.execute_reply":"2024-05-11T23:37:36.947002Z","shell.execute_reply.started":"2024-05-11T23:37:36.601447Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of images: 0\n"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Display 2 sample images from the directory\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mdisplay_normalized_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalizedImages_data_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mdisplay_normalized_images\u001b[1;34m(directory, num_samples)\u001b[0m\n\u001b[0;32m     17\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, num_samples, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m---> 19\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoramilzed image Image \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     21\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]},{"data":{"image/png":"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","text/plain":["<Figure size 1200x600 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Function to display normalized images\n","def display_normalized_images(directory, num_samples=2):\n","    images = []\n","    # Iterate over the directory and select sample images\n","    for root, _, filenames in os.walk(directory):\n","        for filename in filenames:\n","            image_path = os.path.join(root, filename)\n","            images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n","            if len(images) == num_samples:\n","                break\n","        if len(images) == num_samples:\n","            break\n","    \n","    print(\"Number of images:\", len(images))\n","    \n","    # Display the selected images\n","    fig, axes = plt.subplots(1, num_samples, figsize=(12, 6))\n","    for i in range(num_samples):\n","        axes[i].imshow(images[i], cmap='gray')\n","        axes[i].set_title('Noramilzed image Image {}'.format(i+1))\n","        axes[i].axis('off')\n","    plt.show()\n","\n","# Display 2 sample images from the directory\n","display_normalized_images(normalizedImages_data_directory, num_samples=2)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["image_files = os.listdir('normalized_images_equal_ratio/NORMAL')\n","\n","# Select 1200 random indices\n","selected_indices = random.sample(range(len(image_files)), min(1260, len(image_files)))\n","\n","# Use these indices to select the corresponding images\n","selected_images = [image_files[i] for i in selected_indices]\n","\n","# Define the directory\n","dir = 'normalized_images_equal_ratio/NORMAL'\n","\n","# Delete the images that are not in the selected_images list\n","for file in os.listdir(dir):\n","    if file not in selected_images and os.path.isfile(os.path.join(dir, file)):\n","        os.remove(os.path.join(dir, file))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["image_files = os.listdir('normalized_images_equal_ratio/PNEUMONIA')\n","\n","# Select 1200 random indices\n","selected_indices = random.sample(range(len(image_files)), min(1260, len(image_files)))\n","\n","# Use these indices to select the corresponding images\n","selected_images = [image_files[i] for i in selected_indices]\n","\n","# Define the directory\n","dir = 'normalized_images_equal_ratio/PNEUMONIA'\n","\n","# Delete the images that are not in the selected_images list\n","for file in os.listdir(dir):\n","    if file not in selected_images and os.path.isfile(os.path.join(dir, file)):\n","        os.remove(os.path.join(dir, file))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 41s 0us/step\n","WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Found 2520 images belonging to 2 classes.\n","Found 16 images belonging to 2 classes.\n","Epoch 1/5\n","WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From C:\\Users\\Rameen-Laptop\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n"," 6/79 [=>............................] - ETA: 26:00 - loss: 2.1598 - accuracy: 0.6979"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 44\u001b[0m\n\u001b[0;32m     35\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m validation_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     36\u001b[0m     normalizedImages_data_directory_valid,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#validation_data_dir,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from keras.applications.resnet50 import ResNet50\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","\n","class_names = ['PNEUMONIA', 'NORMAL']\n","\n","# Load pre-trained ResNet50 model\n","base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add new layers for our specific task\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","x = Dense(len(class_names), activation='softmax')(x)\n","\n","# Create the final model\n","model = Model(inputs=base_model.input, outputs=x)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Data generators for training and validation\n","train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    normalizedImages_data_directory,\n","    #train_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    normalizedImages_data_directory_valid,\n","    #validation_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=5,\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator)\n",")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4987353,"sourceId":8385559,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
